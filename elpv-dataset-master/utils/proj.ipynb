{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "Here is a description of the data set, which has three columns with the names of the images. There is a probability and whether the label is mono crystal or poly crystal. There are only four types of probabilities,0,0.3,0.6, and 1. So this can be turned into a multi-classification task. Next came the task of dividing the data set. First, we used all the data sets for training, but the result was obviously not very good. Later, we also divided the data set, taking the latter 200 hundred data as the verification set, and all the previous data as the training and test questions. After the division of all data sets was completed, we divided the data set according to label: mano and poly, and verified the last 100 pieces of each data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore ways to deal with class imbalance:\n",
    "\n",
    "Because there are four different probabilities in the data. So we divide all the probabilities into four labels. But because each probability is given a different number, there is an imbalance in the data. For the data imbalance, we did a pre-processing on him. The usual pre-processing used is resampling, and we used resampling here to found good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a classification task, SVM is a good model for classification tasks. But because the SVM model is very simple, my innovation is to do a feature engineering of the picture. \n",
    "First, I SIFT the images. However, the result was not good, although I added the selection of key points after adjusting the contrast and brightness of the images in sift method. \n",
    "The number of key points has increased,but the results are still not very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I carried out hog processing on the image, and adjusted the contrast and brightness, but the brightness of contrast did not have a particularly obvious effect on hog feature extraction. However, hog has the best effect compared with all image feature engineering extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I also processed the image with OTSU. The performance effect is not as good as hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since both random forest and logistic regression are used for classification tasks, we have also written two moduli for comparison with SVM, and the results are inferior to SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion part: \n",
    "\n",
    "I think the SIFT method is a failure, because the accuracy obtained is lower than the original one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTSU\n",
    "\n",
    "Using otsu to extract image features, the effect is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[930  30  14 157]\n",
      " [141  48   6  26]\n",
      " [ 57   5   3  15]\n",
      " [215  27   6 288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.82      0.75      1131\n",
      "           1       0.44      0.22      0.29       221\n",
      "           2       0.10      0.04      0.06        80\n",
      "           3       0.59      0.54      0.56       536\n",
      "\n",
      "    accuracy                           0.64      1968\n",
      "   macro avg       0.46      0.40      0.42      1968\n",
      "weighted avg       0.61      0.64      0.62      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "def otsu_feature_extraction(image):\n",
    "#  otsu fuction\n",
    "    def extract_features(image):\n",
    "        # Apply Otsu thresholding\n",
    "        otsu_threshold = otsu(image)\n",
    "\n",
    "        # Binaryize the image\n",
    "        _, binary_image = cv2.threshold(image, otsu_threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        return binary_image\n",
    "\n",
    "    def otsu(image):\n",
    "        hist, _ = np.histogram(image.ravel(), 256, [0,256])\n",
    "        total = image.size\n",
    "        current_max, threshold = 0, 0\n",
    "        total_sum = np.sum(np.arange(256) * hist)\n",
    "        bg_sum, fg_sum, bg_weight, fg_weight = 0, 0, 0, 0\n",
    "        for i in range(256):\n",
    "            bg_weight += hist[i]\n",
    "            fg_weight = total - bg_weight\n",
    "            if bg_weight == 0 or fg_weight == 0:\n",
    "                continue\n",
    "            bg_sum += i * hist[i]\n",
    "            fg_sum = total_sum - bg_sum\n",
    "            bg_mean = bg_sum / bg_weight\n",
    "            fg_mean = fg_sum / fg_weight\n",
    "            # Calculate between class variance\n",
    "            var_between = bg_weight * fg_weight * (bg_mean - fg_mean) ** 2\n",
    "            # Check if new maximum found\n",
    "            if var_between > current_max:\n",
    "                current_max = var_between\n",
    "                threshold = i\n",
    "        return threshold\n",
    "    \n",
    "    features = extract_features(image)\n",
    "    return features    \n",
    "\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by otsu\n",
    "proc = np.array([otsu_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = proc.reshape(proc.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT\n",
    "\n",
    "sift is used to extract image features without increasing contrast and brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[856  61   4 210]\n",
      " [160  19   2  40]\n",
      " [ 57   3   1  19]\n",
      " [273  23   2 238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      1131\n",
      "           1       0.18      0.09      0.12       221\n",
      "           2       0.11      0.01      0.02        80\n",
      "           3       0.47      0.44      0.46       536\n",
      "\n",
      "    accuracy                           0.57      1968\n",
      "   macro avg       0.35      0.32      0.32      1968\n",
      "weighted avg       0.52      0.57      0.54      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elpv_reader import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "def sift_feature_extraction(image, num_keypoints=100):\n",
    "    sift_class = cv2.SIFT_create()\n",
    "    keypts_l, keypts_d = sift_class.detectAndCompute(image, None)\n",
    "    \n",
    "    # # If fewer than num_keypoints are detected, fill them with zeros\n",
    "    if keypts_d is not None:\n",
    "        if keypts_d.shape[0] < num_keypoints:\n",
    "            keypts_d = np.pad(keypts_d, ((0, num_keypoints - keypts_d.shape[0]), (0, 0)), 'constant')\n",
    "        \n",
    "        else:\n",
    "            keypts_d = keypts_d[:num_keypoints, :] # Truncate if more than num_keypoints are detected\n",
    "    else:\n",
    "        # If no feature points are detected, an all-zero matrix is created\n",
    "        keypts_d = np.zeros((num_keypoints, 128))  \n",
    "\n",
    "    return keypts_d\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by sift\n",
    "proc = np.array([sift_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = proc.reshape(proc.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG\n",
    "\n",
    "Feature extraction is carried out by hog\n",
    "\n",
    "The best effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1003   21    8   99]\n",
      " [ 172   27    0   22]\n",
      " [  55    4    1   20]\n",
      " [ 208   24    2  302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.89      0.78      1131\n",
      "           1       0.36      0.12      0.18       221\n",
      "           2       0.09      0.01      0.02        80\n",
      "           3       0.68      0.56      0.62       536\n",
      "\n",
      "    accuracy                           0.68      1968\n",
      "   macro avg       0.46      0.40      0.40      1968\n",
      "weighted avg       0.63      0.68      0.64      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of random forest and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1001    1    0  129]\n",
      " [ 190    0    0   31]\n",
      " [  56    0    0   24]\n",
      " [ 226    1    0  309]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.89      0.77      1131\n",
      "           1       0.00      0.00      0.00       221\n",
      "           2       0.00      0.00      0.00        80\n",
      "           3       0.63      0.58      0.60       536\n",
      "\n",
      "    accuracy                           0.67      1968\n",
      "   macro avg       0.33      0.37      0.34      1968\n",
      "weighted avg       0.56      0.67      0.61      1968\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "# Use RandomForest classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1023    9    6   93]\n",
      " [ 176   22    1   22]\n",
      " [  54    2    1   23]\n",
      " [ 222   16    3  295]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.79      1131\n",
      "           1       0.45      0.10      0.16       221\n",
      "           2       0.09      0.01      0.02        80\n",
      "           3       0.68      0.55      0.61       536\n",
      "\n",
      "    accuracy                           0.68      1968\n",
      "   macro avg       0.48      0.39      0.39      1968\n",
      "weighted avg       0.64      0.68      0.64      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "# Use logistic regression classifiers\n",
    "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hog feature extraction after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 843  122   36  140]\n",
      " [ 152  919    0   33]\n",
      " [  10   23 1111    0]\n",
      " [ 184   46   37  868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72      1141\n",
      "           1       0.83      0.83      0.83      1104\n",
      "           2       0.94      0.97      0.95      1144\n",
      "           3       0.83      0.76      0.80      1135\n",
      "\n",
      "    accuracy                           0.83      4524\n",
      "   macro avg       0.83      0.83      0.83      4524\n",
      "weighted avg       0.83      0.83      0.83      4524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "images, probs, types = load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "# X = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Over-sampling using RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Create and train the SVM classifier\n",
    "svm_classifier = svm.SVC(C=0.5, kernel='linear', gamma=0.008, decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 200 data sets as verification sets with hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data classification Report:\n",
      "[[900  17   4 102]\n",
      " [149  24   2  23]\n",
      " [ 49   2   3  23]\n",
      " [193   6   1 320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78      1023\n",
      "           1       0.49      0.12      0.19       198\n",
      "           2       0.30      0.04      0.07        77\n",
      "           3       0.68      0.62      0.65       520\n",
      "\n",
      "    accuracy                           0.69      1818\n",
      "   macro avg       0.54      0.41      0.42      1818\n",
      "weighted avg       0.65      0.69      0.65      1818\n",
      "\n",
      "Verify the performance of the set:\n",
      "[[142   0   0   2]\n",
      " [ 30   0   0   1]\n",
      " [  4   0   0   0]\n",
      " [ 19   0   0   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84       144\n",
      "           1       0.00      0.00      0.00        31\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.40      0.10      0.15        21\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.28      0.27      0.25       200\n",
      "weighted avg       0.57      0.72      0.62       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_val = X[-200:]\n",
    "y_val = y[-200:]\n",
    "X_train_test = X[:-200]\n",
    "y_train_test = y[:-200]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.75, random_state=42,stratify=y_train_test)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(\"data classification Report:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred_val = svm_classifier.predict(X_val)\n",
    "print(\"Verify the performance of the set:\")\n",
    "print(confusion_matrix(y_val, y_pred_val))\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG for seperate dataset\n",
    "\n",
    "HOG is used to extract features from images, and the data sets with different labels are trained respectively\n",
    "\n",
    "This shows the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[368   7   0  12]\n",
      " [ 51  24   0   2]\n",
      " [ 32   0   1   4]\n",
      " [ 74   2   3 151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.95      0.81       387\n",
      "           1       0.73      0.31      0.44        77\n",
      "           2       0.25      0.03      0.05        37\n",
      "           3       0.89      0.66      0.76       230\n",
      "\n",
      "    accuracy                           0.74       731\n",
      "   macro avg       0.64      0.49      0.51       731\n",
      "weighted avg       0.74      0.74      0.71       731\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[72  0  0  0]\n",
      " [15  0  0  0]\n",
      " [ 6  0  0  0]\n",
      " [ 7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.18      0.25      0.21       100\n",
      "weighted avg       0.52      0.72      0.60       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly data classification Report:\n",
      "[[583  10   1  53]\n",
      " [ 78  19   0  15]\n",
      " [ 25   1   2   8]\n",
      " [137   7   1 148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.90      0.79       647\n",
      "           1       0.51      0.17      0.26       112\n",
      "           2       0.50      0.06      0.10        36\n",
      "           3       0.66      0.51      0.57       293\n",
      "\n",
      "    accuracy                           0.69      1088\n",
      "   macro avg       0.60      0.41      0.43      1088\n",
      "weighted avg       0.67      0.69      0.66      1088\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[58  0  0  0]\n",
      " [29  0  0  0]\n",
      " [ 2  0  0  0]\n",
      " [10  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74        58\n",
      "           1       0.00      0.00      0.00        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.09      0.17        11\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.40      0.27      0.23       100\n",
      "weighted avg       0.45      0.59      0.45       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "images, probs, types = load_dataset()  \n",
    "\n",
    "# Convert probability to category label\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "images = np.array([hog_feature_extraction(image) for image in images])\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "# Process single crystal data separately\n",
    "X_train_mono, X_test_mono, y_train_mono, y_test_mono = train_test_split(images_mono_train_test, y_mono_train_test, test_size=0.75, random_state=42,stratify=y_mono_train_test)\n",
    "\n",
    "# # Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_train_mono, y_train_mono)\n",
    "\n",
    "# Process polycrystalline data separately\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(images_poly_train_test, y_poly_train_test, test_size=0.75, random_state=42,stratify=y_poly_train_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_test_mono)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_test_mono, y_pred_mono))\n",
    "print(classification_report(y_test_mono, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_test_poly)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_test_poly, y_pred_poly))\n",
    "print(classification_report(y_test_poly, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after hog feature extraction after oversampling after seperating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[313  38  16  25]\n",
      " [ 46 323  11   7]\n",
      " [ 21   0 356   5]\n",
      " [ 70   3  14 300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.74       392\n",
      "           1       0.89      0.83      0.86       387\n",
      "           2       0.90      0.93      0.91       382\n",
      "           3       0.89      0.78      0.83       387\n",
      "\n",
      "    accuracy                           0.83      1548\n",
      "   macro avg       0.84      0.84      0.84      1548\n",
      "weighted avg       0.84      0.83      0.84      1548\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[72  0  0  0]\n",
      " [15  0  0  0]\n",
      " [ 6  0  0  0]\n",
      " [ 6  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.43      0.29      0.27       100\n",
      "weighted avg       0.59      0.73      0.62       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly data classification Report:\n",
      "[[480  76   7  92]\n",
      " [ 41 574   0  19]\n",
      " [  0   0 649   0]\n",
      " [135  53   8 452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       655\n",
      "           1       0.82      0.91      0.86       634\n",
      "           2       0.98      1.00      0.99       649\n",
      "           3       0.80      0.70      0.75       648\n",
      "\n",
      "    accuracy                           0.83      2586\n",
      "   macro avg       0.83      0.83      0.83      2586\n",
      "weighted avg       0.83      0.83      0.83      2586\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[44 14  0  0]\n",
      " [22  7  0  0]\n",
      " [ 2  0  0  0]\n",
      " [ 6  2  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.67        58\n",
      "           1       0.30      0.24      0.27        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.27      0.43        11\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.47      0.32      0.34       100\n",
      "weighted avg       0.54      0.54      0.51       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "images, probs, types = load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "images = np.array([hog_feature_extraction(image) for image in images])\n",
    "# Convert probability to category label\n",
    "\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_mono_resampled, y_mono_resampled = ros.fit_resample(images_mono_train_test, y_mono_train_test)\n",
    "X_mono_train, X_mono_test, y_mono_train, y_mono_test = train_test_split(X_mono_resampled, y_mono_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "ros2 = RandomOverSampler(random_state=42)\n",
    "X_poly_resampled, y_poly_resampled = ros2.fit_resample(images_poly_train_test, y_poly_train_test)\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly_resampled, y_poly_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Process mono crystal data separately\n",
    " # Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_mono_train, y_mono_train)\n",
    "\n",
    "# Process poly crystal data separately\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_poly_train, y_poly_train)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_mono_test)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_mono_test, y_pred_mono))\n",
    "print(classification_report(y_mono_test, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_poly_test)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_poly_test, y_pred_poly))\n",
    "print(classification_report(y_poly_test, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
