{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "Here is a description of the data set, which has three columns with the names of the images. There is a probability and whether the label is single crystal or twin crystal. There are only four types of probabilities, 0,0.3, 0.6, and 1. So this can be turned into a multi-classification task. Next came the task of dividing the data set. First, I used all the data sets for training, but the effect was obviously not very good. Later, I also divided the data set, taking the latter 200 hundred data as the verification set, and all the previous data as the training and test questions. After the division of all data sets was completed, I divided the data set according to label: mano and poly, and verified the last 100 pieces of each data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore ways to deal with class imbalance:(Here's what the training section requires)\n",
    "\n",
    "Because there are four different probabilities in the data. So you can divide all the probabilities into four labels. But because each probability is given a different number, there is an imbalance in the data. For the data imbalance, I can do a pre-processing on him. The usual pre-processing used is resampling, and I used resampling here. Found good effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a classification task, SVM is a good model for classification tasks. But because the SVM model is very simple, my innovation is to do a feature engineering of the picture. First, I SIFT the images. However, the effect was not good, although I added the selection of key points after adjusting the contrast and brightness of the images in sift method. But the results are still not very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I carried out hog processing on the image, and adjusted the contrast and brightness, but the brightness of contrast did not have a particularly obvious effect on hog feature extraction. However, hog has the best effect compared with all image feature engineering extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I also processed the image with OTSU. The performance effect is not as good as hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for why you use these methods, you can use chat to understand. These are just a few common image manipulation methods, so I didn't specifically study why they were chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since both random forest and logistic regression are used for classification tasks, I have also written two moduli for comparison with SVM, and the results are inferior to SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion part: \n",
    "\n",
    "I think the otsu method is a failure, because the accuracy obtained is lower than the original one. If you want to know the reason, you can ask chat, but I haven't explored it further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n",
    "\n",
    "The most basic SVM model starts with four classification tasks, and the probability is divided into four categories of labels. Image feature processing can flatten the image directly, but the result of SVM adjustment is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[910  64  28 129]\n",
      " [112  67   7  35]\n",
      " [ 52   7   2  19]\n",
      " [191  53  26 266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      1131\n",
      "           1       0.35      0.30      0.33       221\n",
      "           2       0.03      0.03      0.03        80\n",
      "           3       0.59      0.50      0.54       536\n",
      "\n",
      "    accuracy                           0.63      1968\n",
      "   macro avg       0.42      0.41      0.41      1968\n",
      "weighted avg       0.62      0.63      0.62      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "   \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "\n",
    "images, probs, types= load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "X = images.reshape(images.shape[0], -1)\n",
    "# Split dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of data setsï¼šOversampling(explore ways to deal with class imbalance.)\n",
    "\n",
    "Oversampling is a common method when dealing with data sets with unbalanced categories. The Oversampling results are much better, but it takes a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 749  165   77  150]\n",
      " [  77  980   14   33]\n",
      " [  10   13 1121    0]\n",
      " [ 220  111   93  711]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68      1141\n",
      "           1       0.77      0.89      0.83      1104\n",
      "           2       0.86      0.98      0.92      1144\n",
      "           3       0.80      0.63      0.70      1135\n",
      "\n",
      "    accuracy                           0.79      4524\n",
      "   macro avg       0.78      0.79      0.78      4524\n",
      "weighted avg       0.78      0.79      0.78      4524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "images, probs, types = load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "X = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Over-sampling using RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Create and train the SVM classifier\n",
    "svm_classifier = svm.SVC(C=0.5, kernel='linear', gamma=0.008, decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after hog feature extraction after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 843  122   36  140]\n",
      " [ 152  919    0   33]\n",
      " [  10   23 1111    0]\n",
      " [ 184   46   37  868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72      1141\n",
      "           1       0.83      0.83      0.83      1104\n",
      "           2       0.94      0.97      0.95      1144\n",
      "           3       0.83      0.76      0.80      1135\n",
      "\n",
      "    accuracy                           0.83      4524\n",
      "   macro avg       0.83      0.83      0.83      4524\n",
      "weighted avg       0.83      0.83      0.83      4524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "images, probs, types = load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "# X = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Over-sampling using RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Create and train the SVM classifier\n",
    "svm_classifier = svm.SVC(C=0.5, kernel='linear', gamma=0.008, decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG\n",
    "\n",
    "Feature extraction is carried out by hog\n",
    "\n",
    "The best effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1003   21    8   99]\n",
      " [ 172   27    0   22]\n",
      " [  55    4    1   20]\n",
      " [ 208   24    2  302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.89      0.78      1131\n",
      "           1       0.36      0.12      0.18       221\n",
      "           2       0.09      0.01      0.02        80\n",
      "           3       0.68      0.56      0.62       536\n",
      "\n",
      "    accuracy                           0.68      1968\n",
      "   macro avg       0.46      0.40      0.40      1968\n",
      "weighted avg       0.63      0.68      0.64      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of random forest and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1001    1    0  129]\n",
      " [ 190    0    0   31]\n",
      " [  56    0    0   24]\n",
      " [ 226    1    0  309]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.89      0.77      1131\n",
      "           1       0.00      0.00      0.00       221\n",
      "           2       0.00      0.00      0.00        80\n",
      "           3       0.63      0.58      0.60       536\n",
      "\n",
      "    accuracy                           0.67      1968\n",
      "   macro avg       0.33      0.37      0.34      1968\n",
      "weighted avg       0.56      0.67      0.61      1968\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "# Use RandomForest classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of logistic regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1023    9    6   93]\n",
      " [ 176   22    1   22]\n",
      " [  54    2    1   23]\n",
      " [ 222   16    3  295]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.79      1131\n",
      "           1       0.45      0.10      0.16       221\n",
      "           2       0.09      0.01      0.02        80\n",
      "           3       0.68      0.55      0.61       536\n",
      "\n",
      "    accuracy                           0.68      1968\n",
      "   macro avg       0.48      0.39      0.39      1968\n",
      "weighted avg       0.64      0.68      0.64      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "  \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by hog\n",
    "X = np.array([hog_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "# Use logistic regression classifiers\n",
    "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT\n",
    "\n",
    "sift is used to extract image features without increasing contrast and brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[856  61   4 210]\n",
      " [160  19   2  40]\n",
      " [ 57   3   1  19]\n",
      " [273  23   2 238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      1131\n",
      "           1       0.18      0.09      0.12       221\n",
      "           2       0.11      0.01      0.02        80\n",
      "           3       0.47      0.44      0.46       536\n",
      "\n",
      "    accuracy                           0.57      1968\n",
      "   macro avg       0.35      0.32      0.32      1968\n",
      "weighted avg       0.52      0.57      0.54      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elpv_reader import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "def sift_feature_extraction(image, num_keypoints=100):\n",
    "    sift_class = cv2.SIFT_create()\n",
    "    keypts_l, keypts_d = sift_class.detectAndCompute(image, None)\n",
    "    \n",
    "    # # If fewer than num_keypoints are detected, fill them with zeros\n",
    "    if keypts_d is not None:\n",
    "        if keypts_d.shape[0] < num_keypoints:\n",
    "            keypts_d = np.pad(keypts_d, ((0, num_keypoints - keypts_d.shape[0]), (0, 0)), 'constant')\n",
    "        \n",
    "        else:\n",
    "            keypts_d = keypts_d[:num_keypoints, :] # Truncate if more than num_keypoints are detected\n",
    "    else:\n",
    "        # If no feature points are detected, an all-zero matrix is created\n",
    "        keypts_d = np.zeros((num_keypoints, 128))  \n",
    "\n",
    "    return keypts_d\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by sift\n",
    "proc = np.array([sift_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = proc.reshape(proc.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Still used SIFT feature extraction,but increase brightness and contrast, and the number of selected key points to compare. There is no significant change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[835 102  13 181]\n",
      " [162  30   2  27]\n",
      " [ 48  12   1  19]\n",
      " [286  48  11 191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68      1131\n",
      "           1       0.16      0.14      0.15       221\n",
      "           2       0.04      0.01      0.02        80\n",
      "           3       0.46      0.36      0.40       536\n",
      "\n",
      "    accuracy                           0.54      1968\n",
      "   macro avg       0.32      0.31      0.31      1968\n",
      "weighted avg       0.50      0.54      0.52      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elpv_reader import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def sift_feature_extraction(image, num_keypoints=500):\n",
    "    adjusted = cv2.convertScaleAbs(image, alpha=2, beta=25)\n",
    "    sift_class = cv2.SIFT_create()\n",
    "    keypts_l, keypts_d = sift_class.detectAndCompute(adjusted, None)\n",
    "    \n",
    "    # If fewer than num_keypoints are detected, fill them with zeros\n",
    "    if keypts_d is not None:\n",
    "        if keypts_d.shape[0] < num_keypoints:\n",
    "            keypts_d = np.pad(keypts_d, ((0, num_keypoints - keypts_d.shape[0]), (0, 0)), 'constant')\n",
    "        else:\n",
    "            # Truncate if more than num_keypoints are detected\n",
    "            keypts_d = keypts_d[:num_keypoints, :]\n",
    "    else:\n",
    "        # If no feature points are detected, an all-zero matrix is created\n",
    "        keypts_d = np.zeros((num_keypoints, 128))\n",
    "\n",
    "    return keypts_d\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by sift\n",
    "proc = np.array([sift_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = proc.reshape(proc.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTSU\n",
    "\n",
    "Using otsu to extract image features, the effect is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[930  30  14 157]\n",
      " [141  48   6  26]\n",
      " [ 57   5   3  15]\n",
      " [215  27   6 288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.82      0.75      1131\n",
      "           1       0.44      0.22      0.29       221\n",
      "           2       0.10      0.04      0.06        80\n",
      "           3       0.59      0.54      0.56       536\n",
      "\n",
      "    accuracy                           0.64      1968\n",
      "   macro avg       0.46      0.40      0.42      1968\n",
      "weighted avg       0.61      0.64      0.62      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "def otsu_feature_extraction(image):\n",
    "#  otsu fuction\n",
    "    def extract_features(image):\n",
    "        # Apply Otsu thresholding\n",
    "        otsu_threshold = otsu(image)\n",
    "\n",
    "        # Binaryize the image\n",
    "        _, binary_image = cv2.threshold(image, otsu_threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        return binary_image\n",
    "\n",
    "    def otsu(image):\n",
    "        hist, _ = np.histogram(image.ravel(), 256, [0,256])\n",
    "        total = image.size\n",
    "        current_max, threshold = 0, 0\n",
    "        total_sum = np.sum(np.arange(256) * hist)\n",
    "        bg_sum, fg_sum, bg_weight, fg_weight = 0, 0, 0, 0\n",
    "        for i in range(256):\n",
    "            bg_weight += hist[i]\n",
    "            fg_weight = total - bg_weight\n",
    "            if bg_weight == 0 or fg_weight == 0:\n",
    "                continue\n",
    "            bg_sum += i * hist[i]\n",
    "            fg_sum = total_sum - bg_sum\n",
    "            bg_mean = bg_sum / bg_weight\n",
    "            fg_mean = fg_sum / fg_weight\n",
    "            # Calculate between class variance\n",
    "            var_between = bg_weight * fg_weight * (bg_mean - fg_mean) ** 2\n",
    "            # Check if new maximum found\n",
    "            if var_between > current_max:\n",
    "                current_max = var_between\n",
    "                threshold = i\n",
    "        return threshold\n",
    "    \n",
    "    features = extract_features(image)\n",
    "    return features    \n",
    "\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "#Process all images by otsu\n",
    "proc = np.array([otsu_feature_extraction(image) for image in images])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = proc.reshape(proc.shape[0], -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42,stratify=y)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 200 data sets as verification sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data classification Report:\n",
      "[[787  64  31 141]\n",
      " [104  65   5  24]\n",
      " [ 40  11   5  21]\n",
      " [199  58   7 256]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      1023\n",
      "           1       0.33      0.33      0.33       198\n",
      "           2       0.10      0.06      0.08        77\n",
      "           3       0.58      0.49      0.53       520\n",
      "\n",
      "    accuracy                           0.61      1818\n",
      "   macro avg       0.43      0.41      0.42      1818\n",
      "weighted avg       0.60      0.61      0.60      1818\n",
      "\n",
      "Verify the performance of the set:\n",
      "[[139   1   0   4]\n",
      " [ 29   2   0   0]\n",
      " [  4   0   0   0]\n",
      " [ 18   0   3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.97      0.83       144\n",
      "           1       0.67      0.06      0.12        31\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.35      0.26      0.24       200\n",
      "weighted avg       0.63      0.70      0.62       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "   \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# Assuming you have a function to load your dataset\n",
    "images, probs, types= load_dataset()\n",
    "\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "X = images.reshape(images.shape[0], -1)\n",
    "# Split dataset into training and testing sets\n",
    "X_val = X[-200:]\n",
    "y_val = y[-200:]\n",
    "X_train_test = X[:-200]\n",
    "y_train_test = y[:-200]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.75, random_state=42,stratify=y_train_test)\n",
    "svm_classifier = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix and classification report\n",
    "print(\"data classification Report:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred_val = svm_classifier.predict(X_val)\n",
    "print(\"Verify the performance of the set:\")\n",
    "print(confusion_matrix(y_val, y_pred_val))\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set was divided according to the label and divided into mono and poly respectively for training, and the effect was better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[344  20   7  16]\n",
      " [ 42  30   3   2]\n",
      " [ 24   0   6   7]\n",
      " [100  19  17  94]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.89      0.77       387\n",
      "           1       0.43      0.39      0.41        77\n",
      "           2       0.18      0.16      0.17        37\n",
      "           3       0.79      0.41      0.54       230\n",
      "\n",
      "    accuracy                           0.65       731\n",
      "   macro avg       0.52      0.46      0.47       731\n",
      "weighted avg       0.66      0.65      0.63       731\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[70  0  0  2]\n",
      " [14  0  0  1]\n",
      " [ 6  0  0  0]\n",
      " [ 7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.83        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.18      0.24      0.21       100\n",
      "weighted avg       0.52      0.70      0.60       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly data classification Report:\n",
      "[[515  55   7  70]\n",
      " [ 51  37   2  22]\n",
      " [ 23   2   2   9]\n",
      " [119  24  14 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76       647\n",
      "           1       0.31      0.33      0.32       112\n",
      "           2       0.08      0.06      0.07        36\n",
      "           3       0.57      0.46      0.51       293\n",
      "\n",
      "    accuracy                           0.63      1088\n",
      "   macro avg       0.42      0.41      0.42      1088\n",
      "weighted avg       0.62      0.63      0.63      1088\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[56  2  0  0]\n",
      " [17 12  0  0]\n",
      " [ 2  0  0  0]\n",
      " [ 9  1  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.97      0.79        58\n",
      "           1       0.80      0.41      0.55        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.09      0.17        11\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.62      0.37      0.38       100\n",
      "weighted avg       0.73      0.69      0.63       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "\n",
    "   \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "images, probs, types = load_dataset()  \n",
    "\n",
    "# Convert probability to category label\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "# Process mono crystal data separately\n",
    "X_train_mono, X_test_mono, y_train_mono, y_test_mono = train_test_split(images_mono_train_test, y_mono_train_test, test_size=0.75, random_state=42,stratify=y_mono_train_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_train_mono, y_train_mono)\n",
    "\n",
    "# Process poly crystal data separately\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(images_poly_train_test, y_poly_train_test, test_size=0.75, random_state=42,stratify=y_poly_train_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_test_mono)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_test_mono, y_pred_mono))\n",
    "print(classification_report(y_test_mono, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_test_poly)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_test_poly, y_pred_poly))\n",
    "print(classification_report(y_test_poly, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample for 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[280  39  35  38]\n",
      " [ 42 314  21  10]\n",
      " [ 21   0 356   5]\n",
      " [ 81  40  24 242]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.69       392\n",
      "           1       0.80      0.81      0.81       387\n",
      "           2       0.82      0.93      0.87       382\n",
      "           3       0.82      0.63      0.71       387\n",
      "\n",
      "    accuracy                           0.77      1548\n",
      "   macro avg       0.77      0.77      0.77      1548\n",
      "weighted avg       0.77      0.77      0.77      1548\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[63  3  5  1]\n",
      " [12  0  1  2]\n",
      " [ 6  0  0  0]\n",
      " [ 5  0  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.18      0.22      0.20       100\n",
      "weighted avg       0.53      0.63      0.57       100\n",
      "\n",
      "poly data classification Report:\n",
      "[[419 111  20 105]\n",
      " [ 52 571   7   4]\n",
      " [  0   0 649   0]\n",
      " [136  64  35 413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       655\n",
      "           1       0.77      0.90      0.83       634\n",
      "           2       0.91      1.00      0.95       649\n",
      "           3       0.79      0.64      0.71       648\n",
      "\n",
      "    accuracy                           0.79      2586\n",
      "   macro avg       0.79      0.79      0.79      2586\n",
      "weighted avg       0.79      0.79      0.79      2586\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[43  7  1  7]\n",
      " [15 13  0  1]\n",
      " [ 1  1  0  0]\n",
      " [ 5  1  0  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.70        58\n",
      "           1       0.59      0.45      0.51        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.38      0.45      0.42        11\n",
      "\n",
      "    accuracy                           0.61       100\n",
      "   macro avg       0.41      0.41      0.41       100\n",
      "weighted avg       0.60      0.61      0.60       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "   \n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "images, probs, types = load_dataset()  \n",
    "\n",
    "# Convert probability to category label\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_mono_resampled, y_mono_resampled = ros.fit_resample(images_mono_train_test, y_mono_train_test)\n",
    "X_mono_train, X_mono_test, y_mono_train, y_mono_test = train_test_split(X_mono_resampled, y_mono_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "ros2 = RandomOverSampler(random_state=42)\n",
    "X_poly_resampled, y_poly_resampled = ros2.fit_resample(images_poly_train_test, y_poly_train_test)\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly_resampled, y_poly_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Process mono crystal data separately\n",
    " # Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_mono_train, y_mono_train)\n",
    "\n",
    "# Process poly crystal data separately\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_poly_train, y_poly_train)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_mono_test)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_mono_test, y_pred_mono))\n",
    "print(classification_report(y_mono_test, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_poly_test)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_poly_test, y_pred_poly))\n",
    "print(classification_report(y_poly_test, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG for seperate dataset\n",
    "\n",
    "HOG is used to extract features from images, and the data sets with different labels are trained respectively\n",
    "\n",
    "This shows the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[368   7   0  12]\n",
      " [ 51  24   0   2]\n",
      " [ 32   0   1   4]\n",
      " [ 74   2   3 151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.95      0.81       387\n",
      "           1       0.73      0.31      0.44        77\n",
      "           2       0.25      0.03      0.05        37\n",
      "           3       0.89      0.66      0.76       230\n",
      "\n",
      "    accuracy                           0.74       731\n",
      "   macro avg       0.64      0.49      0.51       731\n",
      "weighted avg       0.74      0.74      0.71       731\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[72  0  0  0]\n",
      " [15  0  0  0]\n",
      " [ 6  0  0  0]\n",
      " [ 7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.18      0.25      0.21       100\n",
      "weighted avg       0.52      0.72      0.60       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly data classification Report:\n",
      "[[583  10   1  53]\n",
      " [ 78  19   0  15]\n",
      " [ 25   1   2   8]\n",
      " [137   7   1 148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.90      0.79       647\n",
      "           1       0.51      0.17      0.26       112\n",
      "           2       0.50      0.06      0.10        36\n",
      "           3       0.66      0.51      0.57       293\n",
      "\n",
      "    accuracy                           0.69      1088\n",
      "   macro avg       0.60      0.41      0.43      1088\n",
      "weighted avg       0.67      0.69      0.66      1088\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[58  0  0  0]\n",
      " [29  0  0  0]\n",
      " [ 2  0  0  0]\n",
      " [10  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74        58\n",
      "           1       0.00      0.00      0.00        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.09      0.17        11\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.40      0.27      0.23       100\n",
      "weighted avg       0.45      0.59      0.45       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "images, probs, types = load_dataset()  \n",
    "\n",
    "# Convert probability to category label\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "images = np.array([hog_feature_extraction(image) for image in images])\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "# Process single crystal data separately\n",
    "X_train_mono, X_test_mono, y_train_mono, y_test_mono = train_test_split(images_mono_train_test, y_mono_train_test, test_size=0.75, random_state=42,stratify=y_mono_train_test)\n",
    "\n",
    "# # Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_train_mono, y_train_mono)\n",
    "\n",
    "# Process polycrystalline data separately\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(images_poly_train_test, y_poly_train_test, test_size=0.75, random_state=42,stratify=y_poly_train_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_test_mono)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_test_mono, y_pred_mono))\n",
    "print(classification_report(y_test_mono, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_test_poly)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_test_poly, y_pred_poly))\n",
    "print(classification_report(y_test_poly, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test after hog feature extraction after oversampling after seperating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono data classification Report:\n",
      "[[313  38  16  25]\n",
      " [ 46 323  11   7]\n",
      " [ 21   0 356   5]\n",
      " [ 70   3  14 300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.74       392\n",
      "           1       0.89      0.83      0.86       387\n",
      "           2       0.90      0.93      0.91       382\n",
      "           3       0.89      0.78      0.83       387\n",
      "\n",
      "    accuracy                           0.83      1548\n",
      "   macro avg       0.84      0.84      0.84      1548\n",
      "weighted avg       0.84      0.83      0.84      1548\n",
      "\n",
      "Verify the mono performance of the set:\n",
      "[[72  0  0  0]\n",
      " [15  0  0  0]\n",
      " [ 6  0  0  0]\n",
      " [ 6  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84        72\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.43      0.29      0.27       100\n",
      "weighted avg       0.59      0.73      0.62       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly data classification Report:\n",
      "[[480  76   7  92]\n",
      " [ 41 574   0  19]\n",
      " [  0   0 649   0]\n",
      " [135  53   8 452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       655\n",
      "           1       0.82      0.91      0.86       634\n",
      "           2       0.98      1.00      0.99       649\n",
      "           3       0.80      0.70      0.75       648\n",
      "\n",
      "    accuracy                           0.83      2586\n",
      "   macro avg       0.83      0.83      0.83      2586\n",
      "weighted avg       0.83      0.83      0.83      2586\n",
      "\n",
      "Verify the poly performance of the set:\n",
      "[[44 14  0  0]\n",
      " [22  7  0  0]\n",
      " [ 2  0  0  0]\n",
      " [ 6  2  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.67        58\n",
      "           1       0.30      0.24      0.27        29\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       1.00      0.27      0.43        11\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.47      0.32      0.34       100\n",
      "weighted avg       0.54      0.54      0.51       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from elpv_reader import load_dataset\n",
    "from skimage.feature import hog\n",
    "import cv2\n",
    "\n",
    "# Read image\n",
    "def hog_feature_extraction(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image  # If it is already a grayscale image, no conversion is required\n",
    "    \n",
    "\n",
    "# Calculate the HOG feature of the image\n",
    "    fd, hog_image = hog(gray_image, orientations=4, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "def map_probability_to_label(prob):\n",
    "    if prob == 0:\n",
    "        return 0\n",
    "    elif 0 < prob <= 0.4:\n",
    "        return 1\n",
    "    elif 0.4 < prob <= 0.8:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "images, probs, types = load_dataset()\n",
    "y = np.array([map_probability_to_label(prob) for prob in probs])\n",
    "images = np.array([hog_feature_extraction(image) for image in images])\n",
    "# Convert probability to category label\n",
    "\n",
    "# Split the data set by type\n",
    "mono_indices = types == 'mono'\n",
    "poly_indices = types == 'poly'\n",
    "\n",
    "# mono\n",
    "images_mono = images[mono_indices]\n",
    "y_mono = y[mono_indices]\n",
    "images_mono_f = images_mono.reshape(images_mono.shape[0], -1)\n",
    "images_mono_val = images_mono_f[-100:]\n",
    "y_mono_val = y_mono[-100:]\n",
    "images_mono_train_test = images_mono_f[:-100]\n",
    "y_mono_train_test = y_mono[:-100]\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_mono_resampled, y_mono_resampled = ros.fit_resample(images_mono_train_test, y_mono_train_test)\n",
    "X_mono_train, X_mono_test, y_mono_train, y_mono_test = train_test_split(X_mono_resampled, y_mono_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "\n",
    "# poly\n",
    "images_poly = images[poly_indices]\n",
    "y_poly = y[poly_indices]\n",
    "images_poly_f = images_poly.reshape(images_poly.shape[0], -1)\n",
    "images_poly_val = images_poly_f[-100:]\n",
    "y_poly_val = y_poly[-100:]\n",
    "images_poly_train_test = images_poly_f[:-100]\n",
    "y_poly_train_test = y_poly[:-100]\n",
    "\n",
    "ros2 = RandomOverSampler(random_state=42)\n",
    "X_poly_resampled, y_poly_resampled = ros2.fit_resample(images_poly_train_test, y_poly_train_test)\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly_resampled, y_poly_resampled, test_size=0.75, random_state=42)\n",
    "\n",
    "# Process mono crystal data separately\n",
    " # Train the SVM classifier\n",
    "svm_classifier_mono = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_mono.fit(X_mono_train, y_mono_train)\n",
    "\n",
    "# Process poly crystal data separately\n",
    "# Train the SVM classifier\n",
    "svm_classifier_poly = svm.SVC(C=0.5 ,kernel='linear',gamma=0.008,decision_function_shape='ovo')\n",
    "svm_classifier_poly.fit(X_poly_train, y_poly_train)\n",
    "\n",
    "# Predict and evaluate single crystal data\n",
    "y_pred_mono = svm_classifier_mono.predict(X_mono_test)\n",
    "print(\"mono data classification Report:\")\n",
    "print(confusion_matrix(y_mono_test, y_pred_mono))\n",
    "print(classification_report(y_mono_test, y_pred_mono))\n",
    "\n",
    "y_mono_valp = svm_classifier_mono.predict(images_mono_val)\n",
    "print(\"Verify the mono performance of the set:\")\n",
    "print(confusion_matrix(y_mono_val, y_mono_valp))\n",
    "print(classification_report(y_mono_val, y_mono_valp))\n",
    "\n",
    "# Prediction and evaluation of polycrystalline data\n",
    "y_pred_poly = svm_classifier_poly.predict(X_poly_test)\n",
    "print(\"poly data classification Report:\")\n",
    "print(confusion_matrix(y_poly_test, y_pred_poly))\n",
    "print(classification_report(y_poly_test, y_pred_poly))\n",
    "\n",
    "y_poly_valp = svm_classifier_poly.predict(images_poly_val)\n",
    "print(\"Verify the poly performance of the set:\")\n",
    "print(confusion_matrix(y_poly_val, y_poly_valp))\n",
    "print(classification_report(y_poly_val, y_poly_valp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
